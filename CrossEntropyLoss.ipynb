{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Contoh output model untuk 3 sampel (misalnya, 3 gambar)\n",
        "# Setiap baris mewakili satu sampel, dan setiap kolom mewakili skor untuk setiap kelas (kucing, anjing, burung)\n",
        "output = torch.tensor([[2.0, 1.0, 0.1],  # Output untuk sampel 1\n",
        "                       [0.5, 3.0, 1.5],  # Output untuk sampel 2\n",
        "                       [1.2, 0.8, 2.5]]) # Output untuk sampel 3\n",
        "\n",
        "# Label sebenarnya untuk 3 sampel ini\n",
        "# Sampel 1 adalah kucing (kelas 0)\n",
        "# Sampel 2 adalah anjing (kelas 1)\n",
        "# Sampel 3 adalah burung (kelas 2)\n",
        "target = torch.tensor([0, 1, 2])\n",
        "\n",
        "# Inisialisasi fungsi CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Hitung loss\n",
        "loss = criterion(output, target)\n",
        "\n",
        "print(f\"Output model:\\n{output}\")\n",
        "print(f\"Label sebenarnya:\\n{target}\")\n",
        "print(f\"Loss (CrossEntropyLoss): {loss.item():.4f}\")\n",
        "\n",
        "# Mari kita hitung loss secara manual untuk pemahaman yang lebih dalam\n",
        "\n",
        "# 1. Softmax untuk setiap sampel\n",
        "probabilities = F.softmax(output, dim=1) # dim=1 karena kita ingin menerapkan Softmax di sepanjang dimensi kelas\n",
        "print(f\"\\nProbabilitas setelah Softmax:\\n{probabilities}\")\n",
        "\n",
        "# 2. Negative Log Likelihood untuk setiap sampel\n",
        "# Kita perlu mengambil probabilitas dari kelas yang benar untuk setiap sampel\n",
        "# Untuk sampel 1 (target 0): probabilities[0, 0]\n",
        "# Untuk sampel 2 (target 1): probabilities[1, 1]\n",
        "# Untuk sampel 3 (target 2): probabilities[2, 2]\n",
        "\n",
        "log_probabilities = torch.log(probabilities)\n",
        "nll_loss_manual = torch.zeros(target.size(0)) # Inisialisasi tensor untuk menyimpan loss per sampel\n",
        "\n",
        "for i in range(target.size(0)):\n",
        "    nll_loss_manual[i] = -log_probabilities[i, target[i]]\n",
        "\n",
        "print(f\"\\nNegative Log Likelihood Loss (per sampel):\\n{nll_loss_manual}\")\n",
        "\n",
        "# 3. Rata-rata NLL Loss di seluruh sampel\n",
        "mean_nll_loss_manual = torch.mean(nll_loss_manual)\n",
        "print(f\"\\nRata-rata Negative Log Likelihood Loss (manual): {mean_nll_loss_manual.item():.4f}\")\n",
        "\n",
        "# Perhatikan bahwa loss yang dihitung oleh nn.CrossEntropyLoss sama dengan rata-rata NLL Loss manual\n",
        "assert torch.allclose(loss, mean_nll_loss_manual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QJPs5fI4HiN",
        "outputId": "5bd21df4-c98c-42b5-b9f0-952f953133d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output model:\n",
            "tensor([[2.0000, 1.0000, 0.1000],\n",
            "        [0.5000, 3.0000, 1.5000],\n",
            "        [1.2000, 0.8000, 2.5000]])\n",
            "Label sebenarnya:\n",
            "tensor([0, 1, 2])\n",
            "Loss (CrossEntropyLoss): 0.3529\n",
            "\n",
            "Probabilitas setelah Softmax:\n",
            "tensor([[0.6590, 0.2424, 0.0986],\n",
            "        [0.0629, 0.7662, 0.1710],\n",
            "        [0.1873, 0.1255, 0.6872]])\n",
            "\n",
            "Negative Log Likelihood Loss (per sampel):\n",
            "tensor([0.4170, 0.2664, 0.3752])\n",
            "\n",
            "Rata-rata Negative Log Likelihood Loss (manual): 0.3529\n"
          ]
        }
      ]
    }
  ]
}